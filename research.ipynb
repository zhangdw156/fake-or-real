{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c749db1-55d8-448d-986a-0d3c597e8516",
   "metadata": {},
   "source": [
    "## 查看数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f6ff5ed-bd4a-481a-9470-7a3bd5a4ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d4e64f6-b322-4c0d-b6b1-2b3246d91567",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dir='data/train'\n",
    "train_data=pd.read_csv('data/train.csv')\n",
    "pattern_file = r'^file_\\d+\\.txt$'\n",
    "pattern_dir=r'^article_\\d+$'\n",
    "for path in os.listdir(train_dir):\n",
    "    if re.match(pattern_dir, path):\n",
    "        article_path=os.path.join(train_dir,path)\n",
    "        # print(article_path)\n",
    "        for filename in os.listdir(article_path):\n",
    "            if re.match(pattern_file, filename):\n",
    "                # print(filename,int(article_path.split('_')[-1]))\n",
    "                with open(os.path.join(article_path,filename),'r') as f:\n",
    "                    train_data.loc[int(article_path.split('_')[-1]),filename.split('.')[0]]=f.read()\n",
    "train_data.to_json('data/train.json',orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d115b504-2dec-4946-af4c-25e31c51b920",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_dir='data/test'\n",
    "test_data=pd.DataFrame()\n",
    "pattern_file = r'^file_\\d+\\.txt$'\n",
    "pattern_dir=r'^article_\\d+$'\n",
    "for path in os.listdir(test_dir):\n",
    "    if re.match(pattern_dir, path):\n",
    "        article_path=os.path.join(test_dir,path)\n",
    "        # print(article_path)\n",
    "        for filename in os.listdir(article_path):\n",
    "            if re.match(pattern_file, filename):\n",
    "                # print(filename,int(article_path.split('_')[-1]))\n",
    "                with open(os.path.join(article_path,filename),'r') as f:\n",
    "                    test_data.loc[int(article_path.split('_')[-1]),filename.split('.')[0]]=f.read()\n",
    "test_data.to_json('data/test.json',orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1a1923-2dea-40cc-b1b0-3ddbcb2dd8ee",
   "metadata": {},
   "source": [
    "## 准备构造微调数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7043f20e-58b7-4cf6-9945-70aeccf8f0b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>real_text_id</th>\n",
       "      <th>file_1</th>\n",
       "      <th>file_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The VIRSA (Visible Infrared Survey Telescope A...</td>\n",
       "      <td>The China relay network has released a signifi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>China\\nThe goal of this project involves achie...</td>\n",
       "      <td>The project aims to achieve an accuracy level ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Scientists can learn about how galaxies form a...</td>\n",
       "      <td>Dinosaur eggshells offer clues about what dino...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>China\\nThe study suggests that multiple star s...</td>\n",
       "      <td>The importance for understanding how stars evo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>Dinosaur Rex was excited about his new toy set...</td>\n",
       "      <td>Analyzing how fast stars rotate within a galax...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  real_text_id                                             file_1  \\\n",
       "0   0             1  The VIRSA (Visible Infrared Survey Telescope A...   \n",
       "1   1             2  China\\nThe goal of this project involves achie...   \n",
       "2   2             1  Scientists can learn about how galaxies form a...   \n",
       "3   3             2  China\\nThe study suggests that multiple star s...   \n",
       "4   4             2  Dinosaur Rex was excited about his new toy set...   \n",
       "\n",
       "                                              file_2  \n",
       "0  The China relay network has released a signifi...  \n",
       "1  The project aims to achieve an accuracy level ...  \n",
       "2  Dinosaur eggshells offer clues about what dino...  \n",
       "3  The importance for understanding how stars evo...  \n",
       "4  Analyzing how fast stars rotate within a galax...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf67c7e8-0198-445f-b891-54d50d7e3602",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_prompt=''\n",
    "with open('sys_prompt.txt','r') as f:\n",
    "    sys_prompt=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "efc83101-eec6-41e8-966e-04f20073dc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Your task: Compare Text 1 and Text 2, then determine which one is factually correct (true) and which has been modified (false).\\n\\nText 1:\\n<text1>\\n{TEXT1}\\n</text1>\\n\\nText 2:\\n<text2>\\n{TEXT2}\\n</text2>\\n\\nEvaluation criteria:\\n- Check for factual accuracy\\n- Verify logical consistency\\n- Cross-reference with common knowledge\\n\\nIMPORTANT: Your final answer MUST be EITHER \"1\" OR \"2\". No explanations, comments, or additional information. Only the number.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0dca4651-a406-494a-a207-3d2650c91fac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data['prompt'] = train_data.apply(lambda row: sys_prompt.format(TEXT1=row['file_1'],TEXT2=row['file_2']), axis=1)\n",
    "train_data=train_data.rename(columns={'real_text_id':'completion'})\n",
    "train_data['completion']=train_data['completion'].astype(str)\n",
    "train_data=train_data[['prompt','completion']]\n",
    "train_data.to_json('data/train_processed.json',orient='records',lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "81939566-547a-4005-9b84-900f4d94758c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>prompt</th>\n",
       "      <th>completion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Your task: Compare Text 1 and Text 2, then det...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Your task: Compare Text 1 and Text 2, then det...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Your task: Compare Text 1 and Text 2, then det...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Your task: Compare Text 1 and Text 2, then det...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Your task: Compare Text 1 and Text 2, then det...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              prompt completion\n",
       "0  Your task: Compare Text 1 and Text 2, then det...          1\n",
       "1  Your task: Compare Text 1 and Text 2, then det...          2\n",
       "2  Your task: Compare Text 1 and Text 2, then det...          1\n",
       "3  Your task: Compare Text 1 and Text 2, then det...          2\n",
       "4  Your task: Compare Text 1 and Text 2, then det...          2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "777a4d10-5f85-4fb8-ac77-cff693a28774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最长的prompt，id为： 83\n",
      "最长的prompt，长度为： 42413\n"
     ]
    }
   ],
   "source": [
    "longest_idx = train_data['prompt'].str.len().idxmax()\n",
    "print(\"最长的prompt，id为：\",longest_idx)\n",
    "print(\"最长的prompt，长度为：\",len(train_data.loc[longest_idx, 'prompt']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5db9541-8b6d-43ec-be49-1a2a4126423b",
   "metadata": {},
   "source": [
    "## 先尝试推理一波"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dc560b29-579b-4fdb-aaf0-8ea5264af67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir='/data/download-model'\n",
    "model_name='Qwen3-0.6B'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "91e81829-80f9-4ba7-a528-60d1638035d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e5e2b6c-8af2-40ee-9a1b-1dd2ee416259",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path=os.path.join(model_dir,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cdae7aac-05d3-4167-b8ed-c97ab557be99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unsloth import FastLanguageModel\n",
    "# import torch\n",
    "\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = model_path,\n",
    "#     max_seq_length = 40960,   # Context length - can be longer, but uses more memory\n",
    "#     # load_in_4bit = True,     # 4bit uses much less memory\n",
    "#     # load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
    "#     # full_finetuning = False, # We have full finetuning now!\n",
    "#     # token = \"hf_...\",      # use one if using gated models\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a3e92be7-a964-4200-bb6b-489cc25a9a0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline,AutoModelForCausalLM,AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_path)\n",
    "model=AutoModelForCausalLM.from_pretrained(model_path).to('cuda:7')\n",
    "\n",
    "# model=PeftModel.from_pretrained(model,'model/sft')\n",
    "# model=model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3e6a8312-fac6-4146-945b-f5691df2b885",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:7\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline(\n",
    "    task=\"text-generation\",  # 指定任务类型\n",
    "    model=model,             # 传入预加载的模型\n",
    "    max_new_tokens=4,         # 其他参数\n",
    "    tokenizer=tokenizer,\n",
    "    device=7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ef2acf16-fcb5-468f-abb0-5e231f71ff51",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['prompt', 'completion'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_data=pd.read_json('data/train_processed.json',orient='records',lines=True)\n",
    "\n",
    "print(train_data.columns)\n",
    "train_data['text']=train_data.apply(\n",
    "    lambda row:\n",
    "        tokenizer.apply_chat_template(\n",
    "            [\n",
    "                {'role':'user','content':row['prompt']}\n",
    "            ],\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True,\n",
    "            enable_thinking=False\n",
    "        ),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "38c6104f-2a63-4463-983c-4e29579e1ba1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs=generator(train_data['text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0207a91f-84b3-43b0-85a4-a0274624309e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "outputs=[row[0]['generated_text'][len(train_data.loc[idx,'text']):] for idx,row in enumerate(outputs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83245dd1-de88-402d-a477-b34e94d23fd5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data['predict']=outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08268357-4bab-44f8-96a5-2ee3f235ee2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率: 0.46316\n"
     ]
    }
   ],
   "source": [
    "train_data['completion']=train_data['completion'].astype(str)\n",
    "accuracy = (train_data['predict'] == train_data['completion']).mean()\n",
    "print(f\"准确率: {accuracy:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76b388d-5b0b-4cd9-9808-d3559c182812",
   "metadata": {},
   "source": [
    "直接使用0.6B的模型，在训练集只有0.46316的准确率，如果是测试集的话，大概更低一点，距离最高分0.93153还差得远呢\n",
    "- sft， 0.48421？这不是基本没有提升吗？"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89389de9-4942-4e0b-b701-1a31287b8be2",
   "metadata": {},
   "source": [
    "## 尝试直接用sft微调"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffe3cc2b-3f6b-4a4d-b83b-a1e722f8dec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fine/uv/transformers/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from trl import SFTConfig,SFTTrainer\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e2610f71-a571-45da-ab91-4b8929bb58bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_ds=load_dataset('json',data_files='data/train_processed.json',split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9da9c2e8-825f-4ab1-b78f-f62520d18d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion'],\n",
       "    num_rows: 95\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2ba6283f-8b49-49c7-a07d-6e5643346e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "model_dir='/data/download-model'\n",
    "model_name='Qwen3-4B'\n",
    "model_path=os.path.join(model_dir,model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "35855476-d88e-4e29-886f-68bc764234c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from unsloth import FastLanguageModel\n",
    "\n",
    "# model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "#     model_name = model_path,\n",
    "#     # max_seq_length = 40960,   # Context length - can be longer, but uses more memory\n",
    "#     # load_in_4bit = True,     # 4bit uses much less memory\n",
    "#     # load_in_8bit = False,    # A bit more accurate, uses 2x memory\n",
    "#     # full_finetuning = False, # We have full finetuning now!\n",
    "#     # token = \"hf_...\",      # use one if using gated models\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f7aedeab-0a19-4cd1-aab6-f02677d207ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.33it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline,AutoModelForCausalLM,AutoTokenizer\n",
    "from peft import get_peft_model\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_path)\n",
    "model=AutoModelForCausalLM.from_pretrained(model_path).to('cuda:7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa37f347-ccd6-4833-942d-9462b081dff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 源码里有apply_chat_template\n",
    "# train_ds=train_ds.map(lambda example:{\n",
    "#     'prompt':tokenizer.apply_chat_template(\n",
    "#         [\n",
    "#             {'role':'user','content':example['prompt']}\n",
    "#         ],\n",
    "#         tokenize=False,\n",
    "#         add_generation_prompt=True,\n",
    "#         enable_thinking=False\n",
    "#     )}\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f805b960-4ae1-4c29-b9e3-3fb1e7d1ab74",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config=LoraConfig(\n",
    "    r = 16,           # Choose any number > 0! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_alpha = 32,  # Best to choose alpha = rank or rank*2\n",
    "    lora_dropout = 0.1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1e932bc1-2df3-407a-a078-b297395812ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import swanlab\n",
    "\n",
    "swanlab.config.update({\n",
    "    \"model\": model_name\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5de12aa-756e-4da8-87a3-314bbdd8b6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "average_tokens_across_devices is set to True but it is invalid when world size is1. Turn it to False automatically.\n"
     ]
    }
   ],
   "source": [
    "sft_config = SFTConfig(\n",
    "    per_device_train_batch_size = 1,\n",
    "    gradient_accumulation_steps = 1, # Use GA to mimic batch size!\n",
    "    warmup_steps = 5,\n",
    "    num_train_epochs = 10, \n",
    "    learning_rate = 5e-5, \n",
    "    report_to = \"swanlab\", \n",
    "    completion_only_loss=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7f4b3b24-bcb0-476b-b3d3-e68c0255d9f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing train dataset: 100%|██████████| 95/95 [00:00<00:00, 226.87 examples/s]\n",
      "Truncating train dataset: 100%|██████████| 95/95 [00:00<00:00, 26855.76 examples/s]\n",
      "No label_names provided for model class `PeftModel`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    }
   ],
   "source": [
    "trainer=SFTTrainer(\n",
    "    model = model,\n",
    "    processing_class = tokenizer,\n",
    "    train_dataset = train_ds,\n",
    "    args=sft_config,\n",
    "    peft_config=lora_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed5b81d5-ce3e-483f-9dc3-22eb13f348d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 97, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/peft/peft_model.py\", line 818, in forward\n    return self.get_base_model()(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/transformers/utils/generic.py\", line 969, in wrapper\n    output = func(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py\", line 730, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n                                       ^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/transformers/utils/generic.py\", line 969, in wrapper\n    output = func(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py\", line 463, in forward\n    layer_outputs = decoder_layer(\n                    ^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/transformers/modeling_layers.py\", line 48, in __call__\n    return super().__call__(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py\", line 300, in forward\n    hidden_states = self.mlp(hidden_states)\n                    ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py\", line 90, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/peft/tuners/lora/layer.py\", line 727, in forward\n    result = result + lora_B(lora_A(dropout(x))) * scaling\n                                    ^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/dropout.py\", line 70, in forward\n    return F.dropout(input, self.p, self.training, self.inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/functional.py\", line 1425, in dropout\n    _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 94.99 GiB of which 7.31 MiB is free. Process 2156823 has 75.40 GiB memory in use. Including non-PyTorch memory, this process has 19.57 GiB memory in use. Of the allocated memory 18.42 GiB is allocated by PyTorch, and 175.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[40]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uv/transformers/lib/python3.11/site-packages/transformers/trainer.py:2240\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2238\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2239\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2240\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2241\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2242\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2243\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2244\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uv/transformers/lib/python3.11/site-packages/transformers/trainer.py:2555\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2548\u001b[39m context = (\n\u001b[32m   2549\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2550\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2551\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2552\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2553\u001b[39m )\n\u001b[32m   2554\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2555\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2558\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2559\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2560\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2561\u001b[39m ):\n\u001b[32m   2562\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2563\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uv/transformers/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:864\u001b[39m, in \u001b[36mSFTTrainer.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    862\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtraining_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args, **kwargs):\n\u001b[32m    863\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.maybe_activation_offload_context:\n\u001b[32m--> \u001b[39m\u001b[32m864\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uv/transformers/lib/python3.11/site-packages/transformers/trainer.py:3745\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3742\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   3744\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m3745\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3747\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   3748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   3749\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3750\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   3751\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uv/transformers/lib/python3.11/site-packages/trl/trainer/sft_trainer.py:818\u001b[39m, in \u001b[36mSFTTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m    814\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    815\u001b[39m \u001b[33;03mCompute training loss and additionally compute token accuracies\u001b[39;00m\n\u001b[32m    816\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    817\u001b[39m mode = \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.training \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33meval\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m818\u001b[39m (loss, outputs) = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    819\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\n\u001b[32m    820\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m\"\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    822\u001b[39m     \u001b[38;5;66;03m# When using padding-free, the attention_mask is not present in the inputs, instead we have cu_seq_lens_q,\u001b[39;00m\n\u001b[32m    823\u001b[39m     \u001b[38;5;66;03m# cu_seq_lens_k, and max_length_k, max_length_q and position_ids.\u001b[39;00m\n\u001b[32m    824\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mattention_mask\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m inputs:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uv/transformers/lib/python3.11/site-packages/transformers/trainer.py:3810\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   3808\u001b[39m         loss_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   3809\u001b[39m     inputs = {**inputs, **loss_kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m3810\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3811\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   3812\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uv/transformers/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:194\u001b[39m, in \u001b[36mDataParallel.forward\u001b[39m\u001b[34m(self, *inputs, **kwargs)\u001b[39m\n\u001b[32m    192\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.module(*inputs[\u001b[32m0\u001b[39m], **module_kwargs[\u001b[32m0\u001b[39m])\n\u001b[32m    193\u001b[39m replicas = \u001b[38;5;28mself\u001b[39m.replicate(\u001b[38;5;28mself\u001b[39m.module, \u001b[38;5;28mself\u001b[39m.device_ids[: \u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.gather(outputs, \u001b[38;5;28mself\u001b[39m.output_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uv/transformers/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:213\u001b[39m, in \u001b[36mDataParallel.parallel_apply\u001b[39m\u001b[34m(self, replicas, inputs, kwargs)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mparallel_apply\u001b[39m(\n\u001b[32m    211\u001b[39m     \u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any\n\u001b[32m    212\u001b[39m ) -> \u001b[38;5;28mlist\u001b[39m[Any]:\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    214\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    215\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uv/transformers/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:127\u001b[39m, in \u001b[36mparallel_apply\u001b[39m\u001b[34m(modules, inputs, kwargs_tup, devices)\u001b[39m\n\u001b[32m    125\u001b[39m     output = results[i]\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[32m--> \u001b[39m\u001b[32m127\u001b[39m         \u001b[43moutput\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    128\u001b[39m     outputs.append(output)\n\u001b[32m    129\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/uv/transformers/lib/python3.11/site-packages/torch/_utils.py:750\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    747\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    748\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 97, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/peft/peft_model.py\", line 818, in forward\n    return self.get_base_model()(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/transformers/utils/generic.py\", line 969, in wrapper\n    output = func(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py\", line 730, in forward\n    outputs: BaseModelOutputWithPast = self.model(\n                                       ^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/transformers/utils/generic.py\", line 969, in wrapper\n    output = func(self, *args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py\", line 463, in forward\n    layer_outputs = decoder_layer(\n                    ^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/transformers/modeling_layers.py\", line 48, in __call__\n    return super().__call__(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py\", line 300, in forward\n    hidden_states = self.mlp(hidden_states)\n                    ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/transformers/models/qwen3/modeling_qwen3.py\", line 90, in forward\n    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/peft/tuners/lora/layer.py\", line 727, in forward\n    result = result + lora_B(lora_A(dropout(x))) * scaling\n                                    ^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1751, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1762, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/modules/dropout.py\", line 70, in forward\n    return F.dropout(input, self.p, self.training, self.inplace)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/fine/uv/transformers/lib/python3.11/site-packages/torch/nn/functional.py\", line 1425, in dropout\n    _VF.dropout_(input, p, training) if inplace else _VF.dropout(input, p, training)\n                                                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 38.00 MiB. GPU 0 has a total capacity of 94.99 GiB of which 7.31 MiB is free. Process 2156823 has 75.40 GiB memory in use. Including non-PyTorch memory, this process has 19.57 GiB memory in use. Of the allocated memory 18.42 GiB is allocated by PyTorch, and 175.58 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6112e099-02aa-4a2c-82dd-c45dac307658",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_model('model/sft/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f95d20-16d5-45c5-b530-5eebe44a3329",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "transformers"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
