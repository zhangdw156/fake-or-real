The original Data Flow System (DFS) relied on strict calibration procedures using pre-defined accuracy standards for instruments throughout their life cycle within an operational framework ensuring accurate ancillary frame acquisition quality control (QC). Over time this resulted in an established system for monitoring instrumentation detecting performance degradation before it became serious - ultimately producing certified calibration frames used by scientific pipelines for processing observational datasets stored securely within archives .
While this existing system proved reliable , there's now emphasis on expanding its scope beyond just calibrating instruments maintaining stability - aiming towards real-time assessment during observations through direct deployment at telescopes alongside online pipelines . This shift allows comprehensive analysis across multiple aspects like parameter evaluation during acquisition as well as assessing newly designed complex instruments whose raw datasets require sophisticated evaluation methods .
Furthermore , pipeline generation requires consistent evaluation measures ensuring high data quality throughout processing stages . Lastly , comparing traditional calibrated output against simulated results enables closure loop communication between Experimental Testbeds (ETCs) actual hardware performance through APIs enabling timely identification or correction based on discrepancies detected during simulations vs measurements..
This evolution represents significant effort requiring gradual deployment starting mid 2018 until 2023 after which these improvements will continue over time..